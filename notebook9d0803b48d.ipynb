{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Diabet Prediction with Logistic Regression","execution_count":null},{"metadata":{"_uuid":"0241b87ab57776be39237ece19dcbcda5bebacc3"},"cell_type":"markdown","source":"Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6058603d01db5bf83e21a9f16fc3a9a4f488ca6"},"cell_type":"markdown","source":"Let’s first upload the dataset for our analysis:","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7391b45dd213730acd7dfb3d0704b6d7e85e2550"},"cell_type":"markdown","source":"Now print the first five lines to get information about our data. We can see features and any other general information to understand what consist of our dataset.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"8d888130aa445edb8656ff3029e100630ca3e3d1"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c734a071805a24c560385c1f1402bf6b74584b0c"},"cell_type":"markdown","source":"We are seeing details here. All columns names, how many entries it has and if there is, null entries. It is evident from the summary statistic that there are no missing values in the dataset, they are being highlighted as non-null’s. Since we do not have any NaN, we do not need to remove such observations.\n\n","execution_count":null},{"metadata":{"trusted":true,"_uuid":"1432fa761596320753aff325410e82f766b08929"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d596df20208052478f873e228cdcb47b30ffc2d6"},"cell_type":"markdown","source":"This is an alternative piece of code to check whether our dataset has NaN or not.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"6539c82fd543d54b6d36a68bb7574edff631e3cb"},"cell_type":"code","source":"data.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f06f57422d357168437723f90ac6443d0c25d6ba"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dffd84494cd4998982eef55eda61d3a7644f8a5"},"cell_type":"code","source":"data.columns = map(str.lower, data.columns)\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d91a2aaae45181458b406414c48f573a38185a79"},"cell_type":"markdown","source":"See distribution of every features of our dataset:","execution_count":null},{"metadata":{"trusted":true,"_uuid":"97f47c70d09c21b373c4f903e09232aefb409c18"},"cell_type":"code","source":"fig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(data.age, bins = 20, ax=ax[0,0]) \nsns.distplot(data.pregnancies, bins = 20, ax=ax[0,1]) \nsns.distplot(data.glucose, bins = 20, ax=ax[1,0]) \nsns.distplot(data.bloodpressure, bins = 20, ax=ax[1,1]) \nsns.distplot(data.skinthickness, bins = 20, ax=ax[2,0])\nsns.distplot(data.insulin, bins = 20, ax=ax[2,1])\nsns.distplot(data.diabetespedigreefunction, bins = 20, ax=ax[3,0]) \nsns.distplot(data.bmi, bins = 20, ax=ax[3,1]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6dc96075d2f55228b8d57ec1e8e408e1e8aa520"},"cell_type":"code","source":"sns.regplot(x = data.pregnancies, y = data.glucose)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6034def4a2776292c76951e0a4662164bb3035e9"},"cell_type":"code","source":"sns.set(font_scale = 1.15)\nplt.figure(figsize = (14, 10))\n\nsns.heatmap(data.corr(), vmax = 1, linewidths = 0.5, fmt= '.1f',\n            square = True, annot = True, cmap = 'YlGnBu', linecolor = \"white\")\nplt.title('Correlation of Features');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"458c49e4e962524e7e4dece2241de9e1a739477d"},"cell_type":"code","source":"# Normalization\n# Normalization Formula; (x - min(x))/max(x)-min(x)\ny = data.outcome.values\nx = data.drop([\"outcome\"], axis = 1)\n\nx = (x - np.min(x))/(np.max(x)-np.min(x)).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89202d5f6de11e057af97b2deb9401bad7998fa0"},"cell_type":"code","source":"# Train & Test Split\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nfeatures = x_train.T\nlabels = y_train.T\ntest_features = x_test.T\ntest_labels = y_test.T\n\nprint(\"features: \", features.shape)\nprint(\"labels: \", labels.shape)\nprint(\"test_features: \", test_features.shape)\nprint(\"test_labels: \", test_labels.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3d72c66421fdadaa742d4a86b15995184aa9b55"},"cell_type":"code","source":"#Parameter Initialize \ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension, 1),0.01)\n    b= 0.0\n    return w,b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3b1a7522bf29c7937ecf4a79e18c6ccbb85f930"},"cell_type":"code","source":"# Sigmoid Function**\n# Sigmoid Function Formula; 1/(1+e^-x)\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z))\n    return y_head\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8927c511bdd87a2196f706e45bde219f7302d69d"},"cell_type":"code","source":"# Forward & Backward Propagation\ndef foward_and_backward_propagation(w, b, x_train, y_train):\n    #Forward Propagation\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]          #x_train.shape[1] is for scaling\n    \n    # Backward Propagation\n    derivative_weight = (np.dot(x_train, ((y_head-y_train).T)))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost, gradients","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"245c7d4f8337c830458487ea590429bfd31fc2fa"},"cell_type":"code","source":"#Updating Parameters\ndef update(w, b, x_train, y_train, learning_rate, number_of_iterations):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # Updating (learning) parameters is number_of_iterations times\n    for i in range(number_of_iterations):\n        \n        cost, gradients = foward_and_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n        #Let's update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iterations %i: %f\" %(i, cost))\n            \n    # We update (learn) parameters weights and bias\n    parameters = {\"weight\": w, \"bias\": b}\n    plt.plot(index, cost_list2)\n    plt.title(\"Cost-Iteration Relation\")\n    plt.xticks(index, rotation = \"vertical\")\n    plt.xlabel(\"Number of iterations\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cf3a2947960966d4d963a0a92d5e46707ecf22c"},"cell_type":"code","source":"#Prediction\ndef predict(w, b, x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T, x_test)+b)\n    y_prediction = np.zeros((1, x_test.shape[1]))\n    #\n    #\n    for i in range(z.shape[1]):\n        if z[0, i] <= 0.5:\n            y_prediction[0, i] = 0\n        else:\n            y_prediction[0, i] = 1\n            \n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98764f29a647e4c8f3cdf1ae58e31532e232af9c"},"cell_type":"code","source":"# Logistic Regression\ndef logistic_regression(features, labels, test_features, test_labels, learning_rate ,  num_iterations):\n    # Initialize\n    dimension =  features.shape[0]  # It is 8\n    w,b = initialize_weights_and_bias(dimension)\n    parameters, gradients, cost_list = update(w, b, features, labels, learning_rate,num_iterations)\n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],test_features)\n    # Print test errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - test_labels)) * 100))\n    \nlogistic_regression(features, labels, test_features, test_labels,learning_rate = 1.5, num_iterations = 300)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c887726e27dd6f1e132322866e8b63411b3b1095"},"cell_type":"code","source":"# Logistic Regression with Scikit-Learn\nfrom sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\nprint(\"test accuracy: {} \".format(logreg.fit(features.T, labels.T).score(test_features.T, test_labels.T)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c002f1bf0f07775fb5bfc367cecba02567259a0d"},"cell_type":"markdown","source":"**Artificial Neural Networks**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = labels.reshape(labels.shape[0], -1).T\ntest_labels = test_labels.reshape(test_labels.shape[0], -1).T\n\nprint(labels.shape)\nprint(test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1787a97bf212e5e3959b73942adc078d7b8a3d12"},"cell_type":"code","source":"class Artificial_Neural_Network(object):\n    \n    def __init__(self, xTrain, xTest, yTrain, yTest):\n        # Define train and test data\n        self.xTrain = xTrain\n        self.xTest = xTest\n        self.yTrain = yTrain.reshape(yTrain.shape[0],-1)\n        self.yTest = yTest.reshape(yTest.shape[0],-1)\n\n        # Define hyperparameters\n        self.inputLayerSize = self.xTrain.shape[0] # nx <-> Number of samples\n        self.hiddenLayerSize = 4\n        self.outputLayerSize = self.yTrain.shape[0]\n        \n    def initializeWeightsAndBias(self): #, inputLayerSize, hiddenLayerSize, outputLayerSize):\n        \"\"\"\n        This function creates a vector of zeros of shape (inputLayerSize, 1) for w and initializes b to 0.\n\n        Argument:\n        inputLayerSize -- size of the input layer\n        hiddenLayerSize -- size of the hidden layer\n        outputLayerSize -- size of the output layer\n\n        Returns:\n        params -- python dictionary containing your parameters:\n                        W1 -- weight matrix of shape (hiddenLayerSize, inputLayerSize)\n                        b1 -- bias vector of shape (hiddenLayerSize, 1)\n                        W2 -- weight matrix of shape (outputLayerSize, hiddenLayerSize)\n                        b2 -- bias vector of shape (outputLayerSize, 1)\n        \"\"\"\n        np.random.seed(23) # We set up a seed so that your output matches ours \n                           # although the initialization is random.\n        \n        W1 = np.random.randn(self.inputLayerSize, \n                             self.hiddenLayerSize) * 0.01\n        b1 = np.zeros(shape=(self.hiddenLayerSize, 1))\n        W2 = np.random.randn(self.hiddenLayerSize,\n                             self.outputLayerSize) * 0.01\n        b2 = np.zeros(shape=(self.outputLayerSize, 1))\n        \n        # assert(isinstance(B1, float) or isinstance(B1, int))\n        \n        assert (W1.shape == (self.inputLayerSize, self.hiddenLayerSize)), \"[W1] -> Unsuitable matrix size\"\n        assert (b1.shape == (self.hiddenLayerSize, 1))\n        assert (W2.shape == (self.hiddenLayerSize, self.outputLayerSize)), \"[W2] -> Unsuitable matrix size\"\n        assert (b2.shape == (self.outputLayerSize, 1))\n        \n        parameters = {\"W1\": W1,\n                      \"b1\": b1,\n                      \"W2\": W2,\n                      \"b2\": b2}   \n        \n        return parameters\n    \n    def sigmoid(self, Z):\n        \"\"\" Apply and compute sigmoid activation function to scalar, vector, or matrix (Z)\n\n        Arguments:\n        Z -- A scalar or numpy array of any size.\n\n        Return:\n        s -- sigmoid(z)\n        \"\"\"\n        return 1/(1+np.exp(-Z))\n    \n    def forwardPropagation(self, X, parameters):\n        \"\"\" Propogate inputs though network\n        \n        Argument:\n        X -- input data of size (inputLayerSize, m)\n        parameters -- python dictionary containing your parameters (output of initialization function)\n\n        Returns:\n        A2 -- The sigmoid output of the second activation\n        cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n        \"\"\"\n        # Retrieve each parameter from the dictionary \"parameters\"\n        W1 = parameters['W1']\n        b1 = parameters['b1']\n        W2 = parameters['W2']\n        b2 = parameters['b2']\n\n        # Implement Forward Propagation to calculate A2 (probabilities)\n        Z1 = np.dot(W1.T, X) + b1\n        A1 = sigmoid(Z1)\n        Z2 = np.dot(W2.T, A1) + b2\n        yHat = self.sigmoid(Z2) # A2\n        \n        assert(yHat.shape == (1, X.shape[1]))\n    \n        cache = {\"Z1\": Z1,\n                 \"A1\": A1,\n                 \"Z2\": Z2,\n                 \"yHat\": yHat}    # A2\n    \n        return yHat, cache\n    \n    def computeCost(self, yHat, Y, parameters):\n        \"\"\" Compute cost for given X,Y, use weights already stored in class \n\n        Arguments:\n        yHat -- The sigmoid output of the second activation, of shape (1, number of examples)\n        Y -- \"true\" labels vector of shape (1, number of examples)\n        parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n\n        Returns:\n        cost -- cross-entropy cost given equation (13)\n        \"\"\"\n        m = Y.shape[1] # number of example\n                      \n        # Retrieve W1 and W2 from parameters\n        W1 = parameters['W1']\n        W2 = parameters['W2']   \n                    \n        # Loss\n        logprobs = np.multiply(np.log(yHat), Y) + np.multiply((1 - Y), np.log(1 - yHat))\n        # Cost\n        cost = - (np.sum(logprobs)) / m     # m =  yTrain.shape[1]  is for scaling\n        \n        cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n                                    # E.g., turns [[17]] into 17 \n        assert(isinstance(cost, float))\n                      \n        return cost\n\n    def backwardPropagation(self,parameters, cache,  X, Y):\n        \"\"\" Compute the gradients of parameters by implementing the backward propagation\n\n        Arguments:\n        parameters -- python dictionary containing our parameters \n        cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n        X -- input data of shape (2, number of examples)\n        Y -- \"true\" labels vector of shape (1, number of examples)\n\n        Returns:\n        grads -- python dictionary containing your gradients with respect to different parameters\n        \"\"\"\n        m = X.shape[1]   \n                      \n        # First, retrieve W1 and W2 from the dictionary \"parameters\".       \n        W1 = parameters['W1']\n        W2 = parameters['W2']\n                      \n        # Retrieve also A1 and A2 from dictionary \"cache\".\n        A1 = cache['A1']\n        yHat = cache['yHat']                    \n                      \n        # Backward propagation: calculate dW1, db1, dW2, db2.                     \n        dZ2 = yHat - Y\n        dW2 = (1 / m) * np.dot(A1, dZ2.T)\n        db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n        dZ1 = np.multiply(np.dot(W2, dZ2), 1 - np.power(A1, 2))\n        dW1 = (1 / m) * np.dot(X, dZ1.T)#(1 / m) * np.dot(dZ1, self.xTrain.T) # MATRIS BOYUTLARINA BAK dW1 ve dW2 ICIN\n        db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)   # m is for scaling \n\n        gradients = {\"dW1\": dW1,\n                     \"db1\": db1,\n                     \"dW2\": dW2,\n                     \"db2\": db2}\n                      \n        return gradients\n    \n    def updateParameters(self, parameters, gradients, learning_rate = 0.15):\n        \"\"\"\n        Updates parameters using the gradient descent update rule given above\n\n        Arguments:\n        parameters -- python dictionary containing your parameters \n        grads -- python dictionary containing your gradients \n\n        Returns:\n        parameters -- python dictionary containing your updated parameters \n        \"\"\"\n        # Retrieve each parameter from the dictionary \"parameters\"\n        W1 = parameters['W1']\n        b1 = parameters['b1']\n        W2 = parameters['W2']\n        b2 = parameters['b2']\n\n        # Retrieve each gradient from the dictionary \"grads\"\n        dW1 = gradients['dW1']\n        db1 = gradients['db1']\n        dW2 = gradients['dW2']\n        db2 = gradients['db2']\n        \n        # Update rule for each parameter\n        W1 = W1 - learning_rate * dW1\n        b1 = b1 - learning_rate * db1\n        W2 = W2 - learning_rate * dW2\n        b2 = b2 - learning_rate * db2\n\n        parameters = {\"W1\": W1,\n                      \"b1\": b1,\n                      \"W2\": W2,\n                      \"b2\": b2}\n\n        return parameters\n                      \n    def model(self, X, Y, num_iterations=10000, print_cost=False):\n        \"\"\"\n        Arguments:\n        X -- dataset of shape (2, number of examples)\n        Y -- labels of shape (1, number of examples)\n        n_h -- size of the hidden layer\n        num_iterations -- Number of iterations in gradient descent loop\n        print_cost -- if True, print the cost every 1000 iterations\n\n        Returns:\n        parameters -- parameters learnt by the model. They can then be used to predict.\n        \"\"\"\n        np.random.seed(3)\n        \n        costStr = []\n        indexStr = []\n        \n        # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n        parameters = self.initializeWeightsAndBias()\n\n        W1 = parameters['W1']\n        b1 = parameters['b1']\n        W2 = parameters['W2']\n        b2 = parameters['b2']\n \n        # Loop (gradient descent)\n        for i in range(0, num_iterations):\n                      \n            # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n            yHat, cache = self.forwardPropagation(X, parameters)\n\n            # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n            cost = self.computeCost(yHat, Y, parameters)\n            \n            # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n            gradients = self.backwardPropagation(parameters, cache, X, Y)\n\n            # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n            parameters = self.updateParameters(parameters, gradients, learning_rate = 0.0001)\n\n            # Print the cost every 1000 iterations\n            if print_cost and i % 1000 == 0:\n                costStr.append(cost)\n                indexStr.append(i)\n                print (\"Cost after iteration %i: %f\" % (i, cost))\n            \n        # Plot Cost Function\n        plt.plot(indexStr,costStr)\n        plt.xticks(indexStr,rotation='vertical')\n        plt.xlabel(\"Number of Iterarion\")\n        plt.ylabel(\"Cost\")\n        plt.show()\n            \n        return parameters\n\n    def predict(self, parameters, X):\n        \"\"\"\n        Using the learned parameters, predicts a class for each example in X\n\n        Arguments:\n        parameters -- python dictionary containing your parameters \n        X -- input data of size (n_x, m)\n\n        Returns\n        predictions -- vector of predictions of our model (red: 0 / blue: 1)\n        \"\"\"\n        # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n        yHat, cache = self.forwardPropagation(X, parameters)\n        predictions = np.round(yHat)\n\n        \n        return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42ba8a25ec67075f53b1b22e6b957b3242342395"},"cell_type":"code","source":"ANN = Artificial_Neural_Network(features, test_features, labels, test_labels)\nparameters = ANN.model(features, labels, num_iterations = 25000, print_cost=True)\npredictions = ANN.predict(parameters, features)\nprint('Train Accuracy: %d' % float((np.dot(labels, predictions.T) + np.dot(1 - labels, 1 - predictions.T)) / float(labels.size) * 100) + '%')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
